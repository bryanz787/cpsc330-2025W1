{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877e5c4b-301b-4338-ad04-3b62d2264479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979260a-8f2e-4575-b9c0-09f445f24253",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tutorial 3\n",
    "\n",
    "UBC 2025-26\n",
    "\n",
    "## Outline\n",
    "\n",
    "During this tutorial, we will focus on preprocessing - the necessary steps to perform to make the data meaningful for a learning algorithm.\n",
    "\n",
    "All questions can be discussed with your classmates and the TAs - this is not a graded exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc3342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "from plotting_functions import *\n",
    "from utils import *\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7a315",
   "metadata": {},
   "source": [
    "## `ColumnTransformer` on the California housing dataset \n",
    "\n",
    "In this notebook, you will practice features preprocessing using the [California housing dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "\n",
    "Let's start by loading the dataset (this is done for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037930ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"../data/housing.csv\")\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f888c",
   "metadata": {},
   "source": [
    "Let's also add some new features that may help us with the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873eac50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    rooms_per_household=train_df[\"total_rooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    rooms_per_household=test_df[\"total_rooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    bedrooms_per_household=train_df[\"total_bedrooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    bedrooms_per_household=test_df[\"total_bedrooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    population_per_household=train_df[\"population\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    population_per_household=test_df[\"population\"] / test_df[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656d9ed",
   "metadata": {},
   "source": [
    "Finally, we are separating for you the target from the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1533a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's keep both numeric and categorical columns in the data.\n",
    "X_train = train_df.drop(columns=[\"median_house_value\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd859c-1aac-494b-a4b8-3559ed1937ce",
   "metadata": {},
   "source": [
    "## Step 0: EDA\n",
    "Let's get a sense for our dataset using the strategies we learned about last time. From this information, what kinds of preprocessing steps might we need to take here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159cdce-7fa5-4d12-91db-2151255c53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788efd30-f187-4ee8-8d55-e1ebb3418e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"ocean_proximity\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ae8b4-d840-492d-b0c3-33ca28c94c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125b981",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Your turn now! Start by importing ColumnTranformer and make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087ae34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d941f20",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Next, group features by type (numerical or categorical). You may also want to save the target separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0863cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ace5314",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Create a ColumnTransformer for your features. The transformer should include imputation and scaling for numeric features, and encoding for categorical features (which type of encoding?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f270c-edda-4e63-98ee-4f57581b77e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c455ffa6",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Visualize the transformed training set as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b105ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c838c72",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Finally, let's train a classifier (or even better, for practice, a baseline and another regressor): \n",
    "- create a pipeline with the preprocessor and a regressor of your choice.\n",
    "- use the pipeline to perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c612c8-7682-4fe1-93ff-5690cfaba497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615251e1-0723-45d6-a175-f5bda2f4009b",
   "metadata": {},
   "source": [
    "## <font color='red'>Recap/comprehension questions</font>\n",
    "\n",
    "- Do we have to preprocess the target column too?\n",
    "- If we only plan to use a Decision Tree as classifier, do we still need to scale the numerical features?\n",
    "- If the dataset included an ordinal feature \"Neighbourhood desirability\", with numerical labels 1 (poor), 2 (good) and 3 (excellent), would we need to apply an ordinal encoder to it?\n",
    "- Why do we add the argument `drop=\"if_binary\"` to `OneHotEncoder` when dealing with categorical features with only two possible values? What would be the disadvantages of not doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd94a1-ebef-42fd-bb64-3104225943b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
